{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNpRIb346aG49A0N4w5OunN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LxFzbtcBGkTJ","executionInfo":{"status":"ok","timestamp":1754227677716,"user_tz":-60,"elapsed":18000,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"c4ce1b28-5805-4447-bd8d-4ba929894ba0"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","TST Model Performance Benchmark\n","================================================================================\n","Test device: cpu\n","\n","============================================================\n","Test configuration: Window=200 (2.0s), Patch=20, d_ff=256\n","============================================================\n","\n","Parameter count:\n","  - Total parameters: 268,236\n","  - Trainable parameters: 268,236\n","  - Parameter size: 1.02 MB (FP32)\n","\n","Model structure:\n","  - Number of patches: 10\n","  - Sequence length (incl. CLS): 11\n","Please install the 'thop' library to compute FLOPs: pip install thop\n","\n","Inference latency test:\n","Warming up (50 runs)…\n","Measuring latency (200 runs)…\n","\n","  Batch Size = 1:\n","    - Mean latency: 1.462 ± 0.642 ms\n","    - Median: 1.283 ms\n","    - P95: 2.551 ms\n","    - P99: 3.668 ms\n","    - Range: [1.242, 7.072] ms\n","  Batch Size = 16: 0.286 ms/sample\n","  Batch Size = 32: 0.322 ms/sample\n","  Batch Size = 64: 0.364 ms/sample\n","\n","============================================================\n","Test configuration: Window=500 (5.0s), Patch=25, d_ff=256\n","============================================================\n","\n","Parameter count:\n","  - Total parameters: 285,516\n","  - Trainable parameters: 285,516\n","  - Parameter size: 1.09 MB (FP32)\n","\n","Model structure:\n","  - Number of patches: 20\n","  - Sequence length (incl. CLS): 21\n","Please install the 'thop' library to compute FLOPs: pip install thop\n","\n","Inference latency test:\n","Warming up (50 runs)…\n","Measuring latency (200 runs)…\n","\n","  Batch Size = 1:\n","    - Mean latency: 1.528 ± 0.261 ms\n","    - Median: 1.447 ms\n","    - P95: 1.913 ms\n","    - P99: 2.871 ms\n","    - Range: [1.397, 3.269] ms\n","  Batch Size = 16: 0.487 ms/sample\n","  Batch Size = 32: 0.441 ms/sample\n","  Batch Size = 64: 0.425 ms/sample\n","\n","============================================================\n","Test configuration: Window=800 (8.0s), Patch=40, d_ff=320\n","============================================================\n","\n","Parameter count:\n","  - Total parameters: 368,460\n","  - Trainable parameters: 368,460\n","  - Parameter size: 1.41 MB (FP32)\n","\n","Model structure:\n","  - Number of patches: 20\n","  - Sequence length (incl. CLS): 21\n","Please install the 'thop' library to compute FLOPs: pip install thop\n","\n","Inference latency test:\n","Warming up (50 runs)…\n","Measuring latency (200 runs)…\n","\n","  Batch Size = 1:\n","    - Mean latency: 1.692 ± 0.444 ms\n","    - Median: 1.579 ms\n","    - P95: 2.117 ms\n","    - P99: 3.147 ms\n","    - Range: [1.502, 6.135] ms\n","  Batch Size = 16: 0.559 ms/sample\n","  Batch Size = 32: 0.514 ms/sample\n","  Batch Size = 64: 0.681 ms/sample\n","\n","================================================================================\n","Performance Summary\n","================================================================================\n","+-----------+---------+---------+----------+-------------+----------------+\n","| Window(s) | Patches |  Params | FLOPs(M) | Latency(ms) | Throughput(Hz) |\n","+-----------+---------+---------+----------+-------------+----------------+\n","|    2.0    |    10   | 268,236 |   N/A    |  1.46±0.64  |     684.1      |\n","|    5.0    |    20   | 285,516 |   N/A    |  1.53±0.26  |     654.4      |\n","|    8.0    |    20   | 368,460 |   N/A    |  1.69±0.44  |     591.1      |\n","+-----------+---------+---------+----------+-------------+----------------+\n","\n","Memory footprint estimate (FP32):\n","  - Window 2.0s: 1.02 MB\n","  - Window 5.0s: 1.09 MB\n","  - Window 8.0s: 1.41 MB\n"]}],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import time\n","from prettytable import PrettyTable\n","\n","# Copy of the TST model definition\n","class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_dim, num_classes, seq_len, d_model=64,\n","                 n_heads=4, depth=4, d_ff=256, dropout=0.1, patch_size=25):\n","        super().__init__()\n","\n","        self.patch_size = patch_size\n","        self.n_patches = seq_len // patch_size\n","        self.d_model = d_model\n","\n","        # Patch embedding\n","        self.patch_embedding = nn.Linear(input_dim * patch_size, d_model)\n","\n","        # CLS token\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n","\n","        # Position embedding\n","        self.pos_embedding = nn.Parameter(torch.randn(1, self.n_patches + 1, d_model))\n","\n","        # Dropout\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Transformer encoder\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=n_heads,\n","            dim_feedforward=d_ff,\n","            dropout=dropout,\n","            activation='relu',\n","            batch_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n","\n","        # Output layers\n","        self.norm = nn.LayerNorm(d_model)\n","        self.fc_out = nn.Linear(d_model, num_classes)\n","\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        nn.init.xavier_uniform_(self.patch_embedding.weight)\n","        nn.init.xavier_uniform_(self.fc_out.weight)\n","        nn.init.normal_(self.cls_token, std=0.02)\n","        nn.init.normal_(self.pos_embedding, std=0.02)\n","        nn.init.zeros_(self.fc_out.bias)\n","\n","    def forward(self, x):\n","        B, L, D = x.shape\n","\n","        # Create patches\n","        x = x[:, :self.n_patches * self.patch_size, :]\n","        x = x.reshape(B, self.n_patches, self.patch_size * D)\n","\n","        # Patch embedding\n","        x = self.patch_embedding(x)\n","\n","        # Add CLS token\n","        cls_tokens = self.cls_token.expand(B, -1, -1)\n","        x = torch.cat([cls_tokens, x], dim=1)\n","\n","        # Add position embedding\n","        x = x + self.pos_embedding\n","        x = self.dropout(x)\n","\n","        # Transformer\n","        x = self.transformer(x)\n","\n","        # Get CLS output\n","        cls_output = x[:, 0]\n","        cls_output = self.norm(cls_output)\n","\n","        # Classification\n","        logits = self.fc_out(cls_output)\n","\n","        return logits\n","\n","\n","def count_parameters(model):\n","    \"\"\"Calculate the number of model parameters.\"\"\"\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    return total_params, trainable_params\n","\n","\n","def measure_latency(model, input_shape, device, warmup=50, num_runs=200):\n","    \"\"\"Measure model inference latency.\"\"\"\n","    model.eval()\n","    model = model.to(device)\n","\n","    # Create test input\n","    test_input = torch.randn(1, *input_shape).to(device)\n","\n","    # Warm-up\n","    print(f\"Warming up ({warmup} runs)…\")\n","    for _ in range(warmup):\n","        with torch.no_grad():\n","            _ = model(test_input)\n","\n","    # Synchronize CUDA\n","    if device.type == 'cuda':\n","        torch.cuda.synchronize()\n","\n","    # Measure latency\n","    print(f\"Measuring latency ({num_runs} runs)…\")\n","    times = []\n","\n","    for _ in range(num_runs):\n","        if device.type == 'cuda':\n","            torch.cuda.synchronize()\n","            start = torch.cuda.Event(enable_timing=True)\n","            end = torch.cuda.Event(enable_timing=True)\n","\n","            start.record()\n","            with torch.no_grad():\n","                _ = model(test_input)\n","            end.record()\n","\n","            torch.cuda.synchronize()\n","            times.append(start.elapsed_time(end))  # milliseconds\n","        else:\n","            start = time.perf_counter()\n","            with torch.no_grad():\n","                _ = model(test_input)\n","            end = time.perf_counter()\n","            times.append((end - start) * 1000)  # convert to milliseconds\n","\n","    times = np.array(times)\n","    return {\n","        'mean': np.mean(times),\n","        'std': np.std(times),\n","        'median': np.median(times),\n","        'min': np.min(times),\n","        'max': np.max(times),\n","        'p95': np.percentile(times, 95),\n","        'p99': np.percentile(times, 99)\n","    }\n","\n","\n","def calculate_flops(model, input_shape):\n","    \"\"\"Compute model FLOPs.\"\"\"\n","    try:\n","        from thop import profile\n","        dummy_input = torch.randn(1, *input_shape)\n","        flops, params = profile(model.cpu(), inputs=(dummy_input,), verbose=False)\n","        return flops / 1e6  # convert to MFLOPs\n","    except ImportError:\n","        print(\"Please install the 'thop' library to compute FLOPs: pip install thop\")\n","        return None\n","\n","\n","def test_tst_performance():\n","    \"\"\"Benchmark TST model performance.\"\"\"\n","    print(\"=\" * 80)\n","    print(\"TST Model Performance Benchmark\")\n","    print(\"=\" * 80)\n","\n","    # Device setup\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Test device: {device}\")\n","\n","    # Evaluation configurations (identical to the original code)\n","    configs = [\n","        {\"window_size\": 200, \"patch_size\": 20, \"d_ff\": 256},\n","        {\"window_size\": 500, \"patch_size\": 25, \"d_ff\": 256},\n","        {\"window_size\": 800, \"patch_size\": 40, \"d_ff\": 320}\n","    ]\n","\n","    # Fixed parameters\n","    input_dim = 52   # Feature dimension for the PAMAP2 dataset\n","    num_classes = 12  # Number of activity classes in PAMAP2\n","    batch_sizes = [1, 16, 32, 64]  # Test multiple batch sizes\n","\n","    results = []\n","\n","    for config in configs:\n","        window_size = config['window_size']\n","        patch_size = config['patch_size']\n","        d_ff = config['d_ff']\n","\n","        print(f\"\\n{'=' * 60}\")\n","        print(f\"Test configuration: Window={window_size} ({window_size / 100:.1f}s), \"\n","              f\"Patch={patch_size}, d_ff={d_ff}\")\n","        print('=' * 60)\n","\n","        # Instantiate model\n","        model = TimeSeriesTransformer(\n","            input_dim=input_dim,\n","            num_classes=num_classes,\n","            seq_len=window_size,\n","            d_model=64,\n","            n_heads=4,\n","            depth=4,\n","            d_ff=d_ff,\n","            dropout=0.1,\n","            patch_size=patch_size\n","        )\n","\n","        # Parameter count\n","        total_params, trainable_params = count_parameters(model)\n","        print(\"\\nParameter count:\")\n","        print(f\"  - Total parameters: {total_params:,}\")\n","        print(f\"  - Trainable parameters: {trainable_params:,}\")\n","        print(f\"  - Parameter size: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)\")\n","\n","        # Theoretical computational cost\n","        n_patches = window_size // patch_size\n","        print(\"\\nModel structure:\")\n","        print(f\"  - Number of patches: {n_patches}\")\n","        print(f\"  - Sequence length (incl. CLS): {n_patches + 1}\")\n","\n","        # FLOPs\n","        flops = calculate_flops(model, (window_size, input_dim))\n","        if flops:\n","            print(f\"  - FLOPs: {flops:.2f} M\")\n","\n","        # Inference latency tests\n","        print(\"\\nInference latency test:\")\n","        for batch_size in batch_sizes:\n","            input_shape = (window_size, input_dim)\n","\n","            # Adjust the first dimension to batch_size\n","            test_input = torch.randn(batch_size, *input_shape).to(device)\n","            model = model.to(device)\n","\n","            # Measure batch inference latency\n","            with torch.no_grad():\n","                if device.type == 'cuda':\n","                    torch.cuda.synchronize()\n","\n","                start_time = time.perf_counter()\n","                for _ in range(100):\n","                    _ = model(test_input)\n","\n","                if device.type == 'cuda':\n","                    torch.cuda.synchronize()\n","\n","                total_time = (time.perf_counter() - start_time) * 1000 / 100\n","                per_sample_time = total_time / batch_size\n","\n","            # Detailed latency statistics (only for batch_size = 1)\n","            if batch_size == 1:\n","                latency_stats = measure_latency(model, input_shape, device)\n","\n","                results.append({\n","                    'window_size': window_size,\n","                    'patch_size': patch_size,\n","                    'd_ff': d_ff,\n","                    'total_params': total_params,\n","                    'flops_m': flops,\n","                    'latency_mean': latency_stats['mean'],\n","                    'latency_std': latency_stats['std'],\n","                    'latency_p95': latency_stats['p95']\n","                })\n","\n","                print(f\"\\n  Batch Size = {batch_size}:\")\n","                print(f\"    - Mean latency: {latency_stats['mean']:.3f} ± {latency_stats['std']:.3f} ms\")\n","                print(f\"    - Median: {latency_stats['median']:.3f} ms\")\n","                print(f\"    - P95: {latency_stats['p95']:.3f} ms\")\n","                print(f\"    - P99: {latency_stats['p99']:.3f} ms\")\n","                print(f\"    - Range: [{latency_stats['min']:.3f}, {latency_stats['max']:.3f}] ms\")\n","            else:\n","                print(f\"  Batch Size = {batch_size}: {per_sample_time:.3f} ms/sample\")\n","\n","    # Summary table\n","    print(f\"\\n{'=' * 80}\")\n","    print(\"Performance Summary\")\n","    print('=' * 80)\n","\n","    table = PrettyTable()\n","    table.field_names = [\"Window(s)\", \"Patches\", \"Params\", \"FLOPs(M)\", \"Latency(ms)\", \"Throughput(Hz)\"]\n","\n","    for r in results:\n","        table.add_row([\n","            f\"{r['window_size'] / 100:.1f}\",\n","            f\"{r['window_size'] // r['patch_size']}\",\n","            f\"{r['total_params']:,}\",\n","            f\"{r['flops_m']:.2f}\" if r['flops_m'] else \"N/A\",\n","            f\"{r['latency_mean']:.2f}±{r['latency_std']:.2f}\",\n","            f\"{1000 / r['latency_mean']:.1f}\"\n","        ])\n","\n","    print(table)\n","\n","    # Memory footprint estimate\n","    print(\"\\nMemory footprint estimate (FP32):\")\n","    for r in results:\n","        model_size = r['total_params'] * 4 / 1024 / 1024  # MB\n","        print(f\"  - Window {r['window_size'] / 100:.1f}s: {model_size:.2f} MB\")\n","\n","\n","if __name__ == \"__main__\":\n","    test_tst_performance()"]}]}